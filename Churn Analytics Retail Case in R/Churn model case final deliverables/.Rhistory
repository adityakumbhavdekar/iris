c= c&s/s
c
0.25/0.16
1/3
(4/52)+(4/51)+(4/50)
z.value=xbar-mu0/n
xbar=5516
mu0=5423
sd=979
n=352
z.value=xbar-mu0/n
z.value
z.value=(xbar-mu0)/n
z.value
xbar=5516
mu0=5423
sigma=979
z=(xbar-mu0)/(sigma/sqrt(n))
z
alpha=0.05
z.alpha=qnorm(1-alpha)
-z.alpha  ## Critical value
pval=pnorm(z)
pval    ## lower tail p value
trees<-datasets::trees
trees
summary(trees)
n=length(trees$Girth)
sigma=sd(trees$Girth)
SEmean=sigma/sqrt(n)
SEmean
E=qnorm(0.975)*SEmean
E
xbar=mean(trees$Girth)
xbar+ c(E,-E)
median(a)
(50-70)/15
-20/15
pnorm(50,70,15)
pnorm(50,70,15 lower.tail = 50)
pnorm(50,70,15 lower.tail = 50)
pnorm(50,70,15, lower.tail = 50)
pnorm(50,70,15, lower.tail = T)
qnorm(50,70,15, lower.tail = T)
?pnorm
pnorm(q=50,mean=70,sd=15, lower.tail = T)
library(MASS)
height.response=na.omit(survey$Height)
l=length(height.response)
s=sd(height.response)   ## sample standard deviation
SE=s/sqrt(l)     ## standard error
SE
ME=qt(0.975,l-1)   ## margin of error
ME
xbar=mean(height.response)  ## sample mean
xbar+c(-ME,ME)
xbar=5516
mu0=5423
sigma=979
n=352
z=(xbar-mu0)/(sigma/sqrt(n))
z ##1.78
alpha=0.05
z.alpha=qnorm(1-alpha)
-z.alpha  ## Critical value
pval=pnorm(z)
pval  #0.9626  ## lower tail p value
z.alpha=qnorm(1-alpha)
-z.alpha  ## Critical value
xbar=9900  ## sample mean
mu0=10000 ## Hypothesized value
sigma=120 ##population standard deviation
n=30      ##sample size
z=(xbar-mu0)/(sigma/sqrt(n))
z
alpha=0.05
z.alpha=qnorm(1-alpha)
-z.alpha  ## Critical value
pval=pnorm(z)
pval    ## lower tail p value
2.505166e-06 > 0.05
2.505166e-06 < 0.05
xbar=2.1
mu0=2
n=35
sigma=0.25
alpha=0.05
z=(xbar-mu0)/(sigma/sqrt(n))
z  ## test statistic
z.alpha=qnorm(1-alpha)
-z.alpha
pval=pnorm(z,lower.tail = F)
pval  ## upper tail value
0.008980239 < 0.05
pval  #0.9626  ## lower tail p value
xbar=5516
mu0=5423
sigma=979
n=352
z=(xbar-mu0)/(sigma/sqrt(n))
z ##1.78
alpha=0.05
z.alpha=qnorm(1-alpha)
-z.alpha  ## Critical value
pval=pnorm(z)
pval  #0.9626  ## lower tail p value
0.9626468 > 0.05
xbar=14.6
mu0=15.6
n=35
sigma=2.5
z=(xbar-mu0)/(sigma/sqrt(n))
z  ## test statistic
alpha=0.05
z.half.alpha=qnorm(1-alpha/2)
c(-z.half.alpha,z.half.alpha)
pval=2*pnorm(z)
pval  ## Lower tail value
0.01796048 > 0.05
xbar=9900
mu0=10000
n=30
sigma=125
alpha=0.05
z=(xbar-mu0)/(sigma/sqrt(n))
z
alpha=0.05
t.alpha=qt(1-alpha,df=n-1)
-t.alpha
t.alpha=qnorm(1-alpha)
t.aplha
t.alpha
-t.alpha
pval=pt(z,df=n-1)
pval # lower tail p value
z= (xbar-p0)/(sqrt(p0*(1-p0)/n))
xbar= 85/148
p0=0.6
n=148
z= (xbar-p0)/(sqrt(p0*(1-p0)/n))
z
z.alpha= qnorm(1-alpha)
-z.alpha
pval=pnorm(z)
pval
0.2618676 > 0.05
pnorn(50,70,15, lower tail=T)
pnorm(50,70,15, lower tail=T)
pnorm(50,70,15 lower tail=T)
pnorm(70,50,15 lower tail=T)
pnorm(50,70,12)
pnorm(50,70,12 lower.tail = T)
x=50
mean=70
sd= 15
pnorm(q=50,mean=70,sd=15, lower.tail = T)
z= (x-mean)/sd
z
qnorm(0.001)
alpha=0.001
z.alpha=qnorm(1-alpha)
-z.alpha  ## Critical value
qnorm(0.001)
z=qnorm(0.001)
z
?t.test
a=c(60,63,87,45,92,71,56,87,97,69,90,87,74)
mean(a)
75.23
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(x)
result<-getmode(x)
result
median(a)
b=(sd(a)^2)
b
c=25
sd(c)
cv=(5/15)*100
x=50
mean=70
sd= 15
z= (x-mean)/sd
z
0.0125/0.125 =0.1
xbar=5516
mu0=5423
sigma=979
n=352
z=(xbar-mu0)/(sigma/sqrt(n))
z ##1.78
alpha=0.05
z.alpha=qnorm(1-alpha)
-z.alpha  ## Critical value
pval=pnorm(z)
pval  #0.9626  ## lower tail p value
[1] TRUE
xbar=5516
mu0=5423
sigma=979
n=352
trees<-datasets::trees
trees<-datasets::trees
trees
summary(trees)
n=length(trees$Girth)
sigma=sd(trees$Girth)
SEmean=sigma/sqrt(n)
SEmean
E=qnorm(0.975)*SEmean
E
xbar=mean(trees$Girth)
xbar+ c(E,-E)
datasets::trees
l=length(trees$Girth)
trees<-datasets::trees
n=31
sd=sd(length(trees$Girth))
sd
sigma=sd(length(trees$Girth))
sd
n=length(trees$Girth)
sigma=sd(trees$Girth)
SEmean=sigma/sqrt(n)
SEmean
E=qnorm(0.975)*SEmean
E
xbar=mean(trees$Girth)
xbar+ c(E,-E)
E=qnorm(0.05)*SEmean
E
xbar=mean(trees$Girth)
xbar+ c(E,-E)
trees<-datasets::trees
trees
summary(trees)
n=length(trees$Girth)
sigma=sd(trees$Girth)
SEmean=sigma/sqrt(n)
SEmean
E=qnorm(0.05)*SEmean
E
xbar=mean(trees$Girth)
xbar+ c(E,-E)
E=qnorm(0.05)*SEmean
E
xbar=mean(trees$Girth)
xbar+ c(E,-E)
xbar=mean(trees$Girth)
n=length(trees$Girth)
sigma=sd(trees$Girth)
SEmean=sigma/sqrt(n)
SEmean
E=qnorm(0.05)*SEmean
E
xbar=mean(trees$Girth)
xbar+ c(E,-E)
xbar=mean(trees$Girth)
xbar+ c(E,-E)
library(openxlsx)
xbar=mean(trees$Girth)
getwd()
read.xlsx("FORMULA")
read.xlsx("FORMULA.xlxs")
read.xlsx("LOG.xlxs")
list.files()
read.xlsx("FORMULAS.xlsx")
read.xlsx("FORMULAS.xlsx",sheet="IF")
library(openxlsx)
getwd()
list.files()
read.xlsx("FORMULAS.xlsx",sheet="IF")
list.files()
?bison
install.packages(MongoDB)
?mass
?MASS
install.packages("MASS")
gtwd()
getwd()
list.files()
read.csv("house-rents.csv")
model= lm(rentalrates~age+expense+vac_rate+squarefeet+loc)
model= lm(rentalrates~age+expense+vac_rate+squarefeet+loc)
summary(model)
model= lm(rentalrates ~age+ expenses+ vac_rate+ squarefeet+ loc)
hr=read.csv("house-rents.csv")
model= lm(rentalrates ~age+ expenses+ vac_rate+ squarefeet+ loc)
summary(model)
model= lm(rentalrates~age+expenses+vac_rate+ squarefeet+ loc)
model1=lm(rentalrates~)
model1=lm(rentalrates~*)
library(rattle)
rattle()
hr=read.csv("house-rents.csv")
model<- lm(rentalrates~age+expenses+vac_rate+ squarefeet+ loc)
summary(model)
model<- lm(rentalrates~age+expenses+vac_rate+squarefeet+loc)
summary(model)
names(hr)
model <- lm(rentalrates~age+expenses+vac_rate+squarefeet+loc)
summary(model)
model <- lm("rentalrates~age+expenses+vac_rate+squarefeet+loc")
summary(model)
model <- lm(rentalrates~age+expenses+vac_rate+squarefeet+loc,hr)
summary(model)
setwd("D:/Practice/Time Series")
----------------------------------------------------------------------------
> sa <- read.csv("D:/sales.csv", header=T)
> sa
----------------------------------------------------------------------------
sa <- read.csv("D:/sales.csv", header=T)
sa
----------------------------------------------------------------------------
sa <- read.csv("sales.csv", header=T)
----------------------------------------------------------------------------
sa <- read.csv("sales.csv", header=T)
----------------------------------------------------------------------------
sa= read.csv("sales.csv")
----------------------------------------------------------------------------
getwd()
sa= read.csv("sales.csv")
sa
acf(sa)
pacf (sa)
?acf
arima(sa,order= c(1,1,1))
?arima
arima(sa,order= c(1,1,1))
arima(sa,order= c(2,1,2))
arima(sa,order= c(1,1,0))
arima(sa,order= c(0,1,1))
arima(sa,seasonal = list(order = c(1,1,1), period = NA))
arima(sa,seasonal = list(order = c(2,1,2), period = NA))
arima(sa,seasonal = list(order = c(2,2,2), period = NA))
arima(sa,seasonal = list(order = c(2,2,2), period = NA))
arima(sa,seasonal = list(order = c(1,2,1), period = NA))
arima(sa,seasonal = list(order = c(2,0,2), period = NA))
arima(sa,seasonal = list(order = c(1,0,1), period = NA))
arima(sa, order = c(1, 1, 1), seasonal = list(order = c(1, 2, 1), period = 4))
fit<-arima(sa, order = c(1, 1, 1), seasonal = list(order = c(1, 2, 1), period = 4))
tsdiag(fit)
Box.test(fit$residuals,lag=5)
predict(fit,n.ahead=8)
predict(fit,n.ahead=8)
arima(sa,seasonal = list(order = c(2,2,2), period = NA))
?arma
?arma
?ARMAacf
setwd("D:/case studies/4_Churn Analytics Retail Case in R/4_Churn Analytics Retail Case in R/Churn model case final deliverables")
library(VIM)
library(car)
library(glmnet)
library(SamplingStrata)
library(sampling)
cust_data<-read.csv("CustomerData.csv")
summary(cust_data)
cust_data<-cust_data[,-c(17)]
cust_data$Attr2<-ifelse(is.na(cust_data$Attr2),4565,cust_data$Attr2)
cust_data$Attr2<-ifelse(cust_data$Attr2>=6834,4,
ifelse(cust_data$Attr2>=4544,3,
ifelse(cust_data$Attr2>=2333,2,1)))
cust_data$Attr18<-ifelse(cust_data$Attr18>=152,4,
ifelse(cust_data$Attr18>=104,3,
ifelse(cust_data$Attr18>=55,2,1)))
cust_data$Attr4<-ifelse(cust_data$Attr4>=65,4,
ifelse(cust_data$Attr4>=50,3,
ifelse(cust_data$Attr4>=25,2,1)))
summary(cust_data)
head(cust_data)
Corr_data<- cust_data[,-c(1,3,5,7,8,14,15,17,18)]
head(Corr_data)
corr_matrix.csv <- cor(Corr_data,Corr_data)
corr_matrix.csv
write.csv(corr_matrix.csv, "corr_matrix.csv")
vif_Cust_data.csv <- vif(lm(Responder ~ Attr2+Attr4+Attr7+Attr8+Attr9+
Attr10+Attr11+Attr14+Attr18
, data=cust_data))
write.csv(vif_Cust_data.csv, "vif_Cust_data.csv")
View(cust_data)
dat<-cust_data
head(dat)
bi_var<-apply(dat,2,typeof)
bi_var<-data.frame(colnames(dat),bi_var,flag=1)
row.names(bi_var)<-1:nrow(bi_var)
colnames(bi_var)<-c("variable","var_type","flag")
bi_var$flag[which( bi_var$variable %in% c("Cust_id","Responder"))]<-0
head(bi_var)
bi_var<-bi_var[bi_var$flag==1,]
head(bi_var)
event_rate<-NULL
for ( i in 1:nrow(bi_var))
{
## get the freq table for each var..ensure that the deleted var and numbers
## are in sequence..eg responder should be at 2nd position
aa<-as.matrix(table(dat[,i+2],dat[,2]))
## append var name and the categories in that variable
bb<-cbind(rep(as.character(bi_var$variable[i]),nrow(aa)),row.names(aa))
## merge the name, cat and freq table
aa<-data.frame(cbind(bb,aa))
## append everything in new dataset
event_rate<-rbind(event_rate,aa)
}
colnames(event_rate)<-c("variable","Factor","Non-Res","Res")
head(event_rate)
summ_cust_data.csv<-summary(cust_data)
write.csv(event_rate, "event_rate_tbl3.csv")
cust_data$Attrgrp10<-ifelse(cust_data$Attr10 == 3,1,
ifelse(cust_data$Attr10 %in% c('5','2','4'),2,
ifelse(cust_data$Attr10 %in% c('1'),3,4)))
cust_data$Attrgrp11<-ifelse(cust_data$Attr11%in% c('2','5','4','3'),1,
ifelse(cust_data$Attr11 %in% c('6','1'),2,3))
cust_data$Attrgrp6<-ifelse(cust_data$Attr6%in% c('Unmarried'),1,
ifelse(cust_data$Attr6 %in% c('Married'),2,3))
cust_data$Attrgrp7<-ifelse(cust_data$Attr7%in% c('5','4'),1,
ifelse(cust_data$Attr7 %in% c('6','7','3'),2,3))
cust_data$Attrgrp9<-ifelse(cust_data$Attr9 %in% c('5','4','3'),1,
ifelse(cust_data$Attr9 %in% c('2','7','6','1'),2,3))
head(cust_data)
cust_modeldata<-cust_data[,c(1,2,20,21,22,23,24)]
write.csv(cust_modeldata, "cust_modeldata.csv")
cust_modeldata <- cust_modeldata[order(cust_modeldata$Responder,decreasing = TRUE),]
?strata
training_data =strata(cust_modeldata,c("Responder"),size=c(1400,4200), method="srswor")
training_data<-getdata(cust_modeldata,training_data)
View(training_data)
logistic2= glm(Responder ~ as.factor(Attrgrp6)+as.factor(Attrgrp7)+as.factor(Attrgrp9)+
as.factor(Attrgrp10)+as.factor(Attrgrp11), family = binomial("logit"), data=cust_data)
logistic2= glm(Responder ~ as.factor(Attrgrp6)+as.factor(Attrgrp7)+as.factor(Attrgrp9)+
as.factor(Attrgrp10), family = binomial("logit"), data=cust_data)
summary(logistic2) # display results
summary_model.csv<-summary(logistic2)
write.csv(summary_model.csv, "summary_model.csv")
hosmerlem <- function (y, yhat, g = 10)
{
cutyhat <- cut(yhat, breaks = quantile(yhat, probs = seq(0,1, 1/g)), include.lowest = T)
obs <- xtabs(cbind(1 - y, y) ~ cutyhat)
expect <- xtabs(cbind(1 - yhat, yhat) ~ cutyhat)
chisq <- sum((obs - expect)^2/expect)
P <- 1 - pchisq(chisq, g - 2)
c("X^2" = chisq, Df = g - 2, "P(>Chi)" = P)
}
hosmerlem <- function (y, yhat, g = 10)
{
cutyhat <- cut(yhat, breaks = quantile(yhat, probs = seq(0,1, 1/g)), include.lowest = T)
obs <- xtabs(cbind(1 - y, y) ~ cutyhat)
expect <- xtabs(cbind(1 - yhat, yhat) ~ cutyhat)
chisq <- sum((obs - expect)^2/expect)
P <- 1 - pchisq(chisq, g - 2)
c("X^2" = chisq, Df = g - 2, "P(>Chi)" = P)
}
R_hat<-as.vector(fitted(logistic2))
yhat<-R_hat
y<-cust_data$Responder
hosmerlem(y, yhat)
library(ROCR)
gain.chart <- function(y_hat,y) {
plot(performance(prediction(y_hat,y), "tpr", "rpp"),lwd = 7, main = "Lift Chart")
lines(ecdf((rank(-y_hat)[y == T]) / length(y)),verticals = T, do.points = F, col = "red", lwd = 3)
}
gain.chart(R_hat,cust_data$Responder)
outcome_and_fitted_col<-data.frame(cust_data$Responder, R_hat)
colnames(outcome_and_fitted_col)<-c("Responder","fitted.values")
Concordance = function(outcome_and_fitted_col)
{
#outcome_and_fitted_col = cbind(logistic1$Responder, logistic1$fitted.values)
# get a subset of outcomes where the event actually happened
ones = outcome_and_fitted_col[outcome_and_fitted_col[,1] == 1,]
# get a subset of outcomes where the event didn't actually happen
zeros = outcome_and_fitted_col[outcome_and_fitted_col[,1] == 0,]
# Equate the length of the event and non-event tables
if (length(ones[,1])>length(zeros[,1])) {ones = ones[1:length(zeros[,1]),]}
else {zeros = zeros[1:length(ones[,1]),]}
# Following will be c(ones_outcome, ones_fitted, zeros_outcome, zeros_fitted)
ones_and_zeros = data.frame(ones, zeros)
# initiate columns to store concordant, discordant, and tie pair evaluations
conc = rep(NA, length(ones_and_zeros[,1]))
disc = rep(NA, length(ones_and_zeros[,1]))
ties = rep(NA, length(ones_and_zeros[,1]))
for (i in 1:length(ones_and_zeros[,1])) {
# This tests for concordance
if (ones_and_zeros[i,2] > ones_and_zeros[i,4])
{conc[i] = 1
disc[i] = 0
ties[i] = 0}
# This tests for a tie
else if (ones_and_zeros[i,2] == ones_and_zeros[i,4])
{
conc[i] = 0
disc[i] = 0
ties[i] = 1
}
# This should catch discordant pairs.
else if (ones_and_zeros[i,2] < ones_and_zeros[i,4])
{
conc[i] = 0
disc[i] = 1
ties[i] = 0
}
}
# Here we save the various rates
conc_rate = mean(conc, na.rm=TRUE)
disc_rate = mean(disc, na.rm=TRUE)
tie_rate = mean(ties, na.rm=TRUE)
return(list(concordance=conc_rate, num_concordant=sum(conc), discordance=disc_rate, num_discordant=sum(disc), tie_rate=tie_rate,num_tied=sum(ties)))
}
Concordance_test<-Concordance(outcome_and_fitted_col)
Concordance_test
View(hr)
